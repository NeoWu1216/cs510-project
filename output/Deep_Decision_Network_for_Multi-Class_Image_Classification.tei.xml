<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Decision Network for Multi-Class Image Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Venkatesh</forename><forename type="middle">N</forename><surname>Murthy</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">University of Massachusetts</orgName>
								<address>
									<settlement>Amherst</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Singh</surname></persName>
							<email>2vivek-singh@siemens.com</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">University of Massachusetts</orgName>
								<address>
									<settlement>Amherst</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrence</forename><surname>Chen</surname></persName>
							<email>terrence.chen@siemens.com</email>
							<affiliation key="aff1">
								<orgName type="department">Medical Imaging Technologies</orgName>
								<address>
									<addrLine>Siemens Healthcare</addrLine>
									<settlement>Princeton</settlement>
									<region>NJ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Manmatha</surname></persName>
							<email>manmatha@cs.umass.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">University of Massachusetts</orgName>
								<address>
									<settlement>Amherst</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dorin</forename><surname>Comaniciu</surname></persName>
							<email>dorin.comaniciu@siemens.com</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">University of Massachusetts</orgName>
								<address>
									<settlement>Amherst</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Medical Imaging Technologies</orgName>
								<address>
									<addrLine>Siemens Healthcare</addrLine>
									<settlement>Princeton</settlement>
									<region>NJ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Decision Network for Multi-Class Image Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:39+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we present a novel Deep Decision Network (DDN) that provides an alternative approach towards building an efficient deep learning network. During the learning phase, starting from the root network node, DDN automatically builds a network that splits the data into disjoint clusters of classes which would be handled by the subsequent expert networks. This results in a tree-like structured network driven by the data. The proposed method provides an insight into the data by identifying the group of classes that are hard to classify and require more attention when compared to others. DDN also has the ability to make early decisions thus making it suitable for timesensitive applications. We validate DDN on two publicly available benchmark datasets: CIFAR-10 and CIFAR-100 and it yields state-of-the-art classification performance on both the datasets. The proposed algorithm has no limitations to be applied to any generic classification problems.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Convolutional Neural Network (CNN) based methods have consistently been the top performers on large scale image classification benchmarks such as ImageNet Large-Scale Visual Recognition challenge (ILSVRC 2012, 2013 and 2014) <ref type="bibr" target="#b11">[12]</ref>[24] <ref type="bibr" target="#b28">[29]</ref>. This success of CNNs is partly due to the availability of large datasets and high-performance computing systems and partly due to the recent technical advances on learning methods and regularization techniques like dropout <ref type="bibr" target="#b26">[27]</ref>, dropconnect <ref type="bibr" target="#b30">[31]</ref>, maxout <ref type="bibr" target="#b4">[5]</ref> and batch normalization <ref type="bibr" target="#b7">[8]</ref>. However there are still no well established guidelines to train a performant deep network, and thus, training a deep network often involves thorough experimentation and statistical analysis. Although going deeper * work was carried out during his internship at Siemens in the neural network design has shown to be effective <ref type="bibr" target="#b23">[24]</ref> <ref type="bibr" target="#b28">[29]</ref>. It also increases the training duration as well as the risk of over-fitting.</p><p>We propose a novel computational framework called Deep Decision Network (DDN) to design an efficient deep network architecture without over-fitting the training process. In contrast to existing deep learning based approaches, DDN is built stage-wise during the learning phase. At each stage, the network introduces decision stumps to classify confident samples and partition the remaining data, which is difficult to classify, into smaller data clusters which are used for learning successive expert networks in the next stage. Note that data clusters at each stage are such that the samples within a cluster are difficult to distinguish using the trained classifier at that stage but the samples across clusters are easily distinguishable. This is achieved by fine tuning the trained classifier using a combination of softmax and weighted contrastive loss. While the clustering is motivated by the divide-and-conquer principle, it has the added benefit to automatically discovering a data hierarchy based on appearance similarity. Notice that the DDN implicitly captures the intuition that hard samples require more computation.</p><p>Our contributions are as follows: (a) proposed stagewise training strategy for the DDN helps alleviate problems encountered by gradient based methods on deeper architectures, (b) joint-loss (weighted contrastive and classification) optimization of the network to minimize errors during data partitioning, (c) data-driven design of DDN offers an insight into underlying structure in the data, (b) proposed network architecture can make early decisions thereby reducing computational time without compromising on the performance and finally, (e) DDN achieves state of the art performance on CIFAR-10 and CIFAR-100 <ref type="bibr" target="#b10">[11]</ref> public benchmarks. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Deep Learning in general has shown to be an effective framework advancing the state-of-the art performance on various tasks <ref type="bibr" target="#b21">[22]</ref> in the field of computer vision <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b3">4]</ref>. In particular, CNN based models has been the top performers in computer vision related tasks till date but recent work on Recurrent Neural Network [RNN] <ref type="bibr" target="#b18">[19]</ref> has shown to be effective as well. Many attempts have been made to come up with an effective CNN network architecture either by going deeper <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b28">29]</ref> which were the top performers in the 2014 ImageNet challenge or by introducing new components:-activation units like (a) rectified linear unit (ReLu) <ref type="bibr" target="#b11">[12]</ref> helped in accelerating the learning and have a great influence on the performance of large models trained on large datasets, (b) Parametric rectified linear units (PReLu) <ref type="bibr" target="#b5">[6]</ref> which replaces the parameter-free ReLU activation by a learned parametric activation unit to further improve the classification performance; regularizers like (a) dropout <ref type="bibr" target="#b26">[27]</ref> randomly sets some activation units to zero in a given layer and provides the effect of model averaging, (b) dropconnect <ref type="bibr" target="#b30">[31]</ref> instead of activation units the weights set to zero, (c) maxout <ref type="bibr" target="#b4">[5]</ref> outputs the max of a set of inputs and this can be used as an alternative to dropout; normalization such as batch normalization <ref type="bibr" target="#b7">[8]</ref> that normalizes the layer inputs providing an accelerated learning and improved perfor-mance. In this work, we propose an alternative generic deep learning framework which helps in improving the classification performance by leveraging any of the existing networks (with mixture of new components) as the starting root node in our proposed tree structured deep decision network. This work is inspired by decision trees <ref type="bibr" target="#b19">[20]</ref> and the idea of sample partitioning <ref type="bibr" target="#b1">[2]</ref>, which are both classical approaches in machine learning. Many variants of boosting trees have been explored and shown to be successful for most of the vision tasks <ref type="bibr" target="#b20">[21]</ref>. There has been few related papers that are tree-like structured CNNs, starting with <ref type="bibr" target="#b27">[28]</ref> aimed towards improving the classification performance of classes with limited training dataset by transferring knowledge among similar classes. A recent paper <ref type="bibr" target="#b6">[7]</ref> attempted to build a hierarchical CNN but the main objective was to transfer knowledge from a large network to a small network to achieve scalability but without compromising on the performance. In our proposed work, we aim to provide a generic framework that automatically discovers datahierarchy and improve the performance by separating out the easily separable data from the hard ones. The hard confusion cases will be routed deep down the tree to be handled by the expert network nodes.</p><p>Recent work <ref type="bibr" target="#b31">[32]</ref> which is similar in spirit to our work, with a few but important differences. Firstly, by design it was only two levels but our proposed method can be multi-level driven completely by the data. Secondly, softclustering was used for grouping confusion cases but we use hard clustering with joint-loss optimization to minimize the errors which is shown to be more effective. Thirdly, we propose piece-wise training as opposed to component-wise pre-training followed by global fine-tuning. Fourthly, DDN uses multiple decision layers capable of making early decisions as opposed to using one probabilistic averaging layer and finally our method results on CIFAR-100 is better even without any data-augmentation and 10 average testing <ref type="bibr" target="#b11">[12]</ref> tricks as used in <ref type="bibr" target="#b31">[32]</ref>.</p><p>In comparison to the most recent work, Deep Neural Decision Forests (dNDF) <ref type="bibr" target="#b9">[10]</ref>, at a high level, both our proposed technique and their method deals with a realization of decision trees in the context of CNN, but there are some key differences: a) In dNDF, a decision tree is introduced after the fully connected layer as part of the CNN but in DDN, each node of a decision tree is a CNN. b) Training a dNDF requires the user to set the number of trees, while in DDN, the network hierarchy is data-driven and determined automatically using a fitness measure; unlike a category based decomposition of the data which can be visually examined, the number of trees in a decision forest is difficult to set intuitively. c) The hierarchical data decomposition obtained as a result of training a DDN also provides valuable insights into the data such as hard/easy samples, most confusing set of classes, outliers etc.. d) Since dNDF has not reported the performance on CIFAR-10 or CIFAR-100, a direct quantitative comparison is not feasible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Deep Decision Network Framework</head><p>This section describes the Deep Decision Network and the algorithms involved in learning the deep decision network architecture and it's parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Deep Decision Network</head><p>A deep decision network (DDN) is a tree structured deep neural network with decision stumps at each node to classify easily separable data earlier in the network and to determine the subsequent expert node for the difficult cases. An overview of the DDN computational framework is provided in <ref type="figure" target="#fig_0">Figure 1</ref>.</p><p>Given a dataset, root (level 1) network is trained using the back propagation algorithm. Instead of optimizing the network to obtain the best performance, we only need to optimize until a reasonable performance is achieved e.g. 60-70%. Alternatively, a pre-trained network can be used as a root network if it achieves reasonable performance. The confusion matrix, computed over the validation dataset, is then used to identify clusters of object classes, such that each cluster may have large confusion among classes inside the cluster but the confusion across clusters is low. A subsequent expert network is trained for data within each clus-ter to correctly classify the previously misclassified samples and/or the samples classified with low confidence. This has the effect that as we go deeper we continue to "zoom-in" on resolving the problem cases. This process of building the the network is continued until we see no further improvement on the validation data set. During testing, a sample is routed through DDN until it's class is determined (via early classification or at the leaf node).</p><p>There are a few key differences between the DDN architecture and the traditional deep networks. Firstly, all the layers in the previous levels are frozen while training the newly introduced network layers which forms a new node at the next level. Secondly, each node is built on the parent node's feature space to specifically handle a subset of classes. Note that each node can be trained starting from any layer of the parent node, and this choice of the layer can be determined using a cross validation data set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Discovering data clusters</head><p>Here we discuss how the clusters are identified at each node of the Deep Decision Network using the spectral coclustering algorithm <ref type="bibr" target="#b2">[3]</ref>. The spectral co-clustering algorithm approximates the normalized cut of a bi-partite graph (symmetric matrix) to find heavy sub-graphs (sub-matrices) thus resulting in block diagnolization of the matrix. We apply the spectral co-clustering algorithm over the co-variance of the confusion matrix; each block in the resulting block diagonal matrix forms a cluster. Notice that although different clusters would be disjoint (no overlapping classes) and the confusion among the classes within a cluster would be high. Furthermore, if there are any entries (in the confusion matrix) which are not within the diagonal blocks, then the samples contributing to those entries would get missclassified. Thus, to minimize the likelihood of such missclassifications, we fine tune the network parameters using a joint loss, combining softmax and weighted contrastive loss; this is explained in detail in the Section 3.3.</p><p>In order to determine the optimal clustering C * , we define a fitness measure f m(C), for a given clustering C computed using spectral co-clustering, as</p><formula xml:id="formula_0">f m(C) = ǫ + 1 K K i=1 |C i |<label>(1)</label></formula><p>where, ǫ is the miss-classification error introduced due to the data-split, C i is i th cluster (set of classes), |.| is the size of a set. The optimal clustering C * is then given by,</p><formula xml:id="formula_1">C * = arg min C f m(C)<label>(2)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Minimizing errors during splitting</head><p>As mentioned earlier, errors due to incorrect assignment of samples to the clusters are irrecoverable. For example, let's say if the level 1 root network (trained with softmaxloss) miss-classifies some airplane images with dog and ship classes. Now if airplane and ship classes form a cluster and the airplane image samples that are miss-classified as a dog by the root network, will all be assigned to the incorrect cluster (expert node) which would fail to assign them the correct class (airplane).</p><p>To minimize such miss-classification errors, we augment the softmax-loss with an error-driven, weighted contrastive loss function that helps block diagonalize the confusion matrix; this is depicted in the <ref type="figure" target="#fig_1">Figure 2</ref>. The overall loss function is given by</p><formula xml:id="formula_2">L = λ 2 × L m + λ 1 × L sof tmax<label>(3)</label></formula><p>where, L m is the weighted contrastive loss function. The weights λ 1 and λ 2 were set to 1.0 based on performance on the validation dataset. The weighted contrastive loss L m can be interpreted as a set of soft constraints which impose a significantly higher penalty for miss-classifying a sample to any class belonging to another cluster as compared to the penalty of missclassifying to a class that belongs to the same cluster. In other words, minimizing the weighted contrastive loss results in the similarity metric of samples belonging to the same cluster to be small and samples across different clusters to be large. The weighted contrastive loss function is given by</p><formula xml:id="formula_3">L m = w ij × (1 − Y ) 2 × D 2 + (4) Y 2 × {max(0, m − D)} 2 where, w ij = 0.1 if i ⊂ C k and j ⊂ C k 1 otherwise</formula><p>where, w ij is the weight corresponding to class labels i and j, D is the L 2 -norm between a pair of samples. Y is the label representing whether samples are similar or dissimilar pairs, m is the margin. C k indicates the k th cluster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Piece-wise training for DDN</head><p>The proposed architecture is trained in a unique fashion. Starting with a root network (trained in the traditional way), we use it's softmax layer to compute the performance and learn a classification threshold for each class using crossvalidation. This threshold is used during testing to make an early decision on samples. We then compute the confusion matrix on the validation dataset and use it to identify the clusters of confusion classes (as explained in Section 3.2). Next, the network is fine-tuned using the weighted contrastive loss (as explained in Section 3.3). The weights for the contrastive loss function are determined based on the confusion matrix. After fine-tuning, the samples are split according to their cluster ID's.</p><p>For each cluster, a node is added to the decision network. A node itself is a shallow network (or expert network) trained to distinguish between a subset of classes belonging to that cluster. For an expert network architecture, we utilize the micro networks of NIN <ref type="bibr" target="#b14">[15]</ref>. Note that when we train the new layers, we freeze the previously trained layers by setting their learning rate to zero.</p><p>This process of adding node to the decision network is continued recursively until there is no more improvement on the validation dataset and/or the maximum depth of the network is reached.</p><p>With this training scheme, DDN is able to make use of the efforts in the early layers for training the subsequent layers, and have the benefit of making an early decision. Furthermore, training expert networks (node) starting from parent feature spaces helps in overcoming the over-fitting problem. In addition, it also helps in avoiding getting stuck in poor solutions during the gradient optimization process, and converges to network parameters that provide better generalization; this is validated in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Experimental Validation on MNIST digits</head><p>To validate DDN, we carried out a controlled experiment on a subset of MNIST <ref type="bibr" target="#b12">[13]</ref> dataset. We chose LeNet as the starting root node at level-1 and for every subsequent level expert nodes, we added a convolution layer and a fully connected layer (going deeper but to handle only a subset of the data which are considered to be the hard ones). From the confusion matrix produced at level-1 we identified digits '6' and '8' forming a cluster (confusing class set) and some of these confusions are shown in In <ref type="figure" target="#fig_2">Figure 3</ref>. To address these confusions, we build an expert network node at level-2, which is built upon level-1 feature space. <ref type="figure" target="#fig_2">From Figure 3</ref> it is clear that the mistakes are significantly reduced at level-2. Note that, since the resulting network is datadriven, the stopping criterion for network-growth is when the subsequent network fails to discriminate or improve on the validation dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Classification using DDN</head><p>Given an image, we feedforward it through the root node at level-1 of the DDN and obtain the confidence score from the softmax layer. If the score is higher than the threshold value (determined during the training process) then we declare it as the final output. If not, the sample gets routed to the appropriate branch of the network based on its prediction label and the process is repeated either until the prediction score is higher than the confidence value or until it reaches the leaf node to get the final response. </p><formula xml:id="formula_4">f (I) =            y if (Î sj=1 = f sj=1 (I)) &gt; T sj=1 {i} y if (Î sj=2 = f sj=2 (Î sj=1 )) &gt; T sj=2 {i} . . .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results</head><p>We report the performance of the proposed method in comparison to other methods on publicly available benchmark datasets -CIFAR-10 and CIFAR-100 <ref type="bibr" target="#b10">[11]</ref>. We implemented our method using Caffe <ref type="bibr" target="#b8">[9]</ref> and all the experiments were carried out on a single Titan-X GPU. The train-test splits and data pre-processing are as provided in <ref type="bibr" target="#b4">[5]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Network Details</head><p>In this work, we chose NIN as the root node of our DDN for experimenting on both CIFAR-10 and CIFAR-100 datasets. The root node could be any existing network but we chose NIN because it was shown to yield state-of-the performance on both the datasets. NIN has also has the nice property of being built with mlpconv (MLP) has a basic building block unit. The original NIN consists of three MLP layers. Each MLP layer is composed of a three-layer perceptron and a pooling layer. DDN consists of NIN as the root node and additional layers (shallow-network/branch nodes) are simply one mlpconv layer of NIN. Additional layers were introduced right after the second mlpconv unit of NIN to make use of the local feature response instead of the third node which seems to capture global class specific features. As in NIN, global average pooling was used instead of fully connected layers at the leaf nodes.</p><p>All the network parameter settings, weights initialization and learning policy strictly follow the settings provided by NIN. The only change was during the addition of new layers (shallow-networks), the learning rate was set to 0.01 with a step size of 25K. In the current setup for both the datasets, we had only two levels, with root node NIN at level-1 and multiple MLP units in level-2. Each MLP was specialized to address a particular cluster consisting of the most confusing classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">CIFAR-10</head><p>Experimental Setup: The CIFAR-10 dataset <ref type="bibr" target="#b10">[11]</ref> consists of 10 classes of natural images with a total of 50K training images and a total of 10K testing images. Each image is of size 32x32 and we follow the same pre-processing of global contrast normalization and ZCA whitening as in <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b14">15]</ref>. For the validation dataset, we used the last 10K samples of the training to determine the confidence level threshold and data splits based on the confusion matrix. After determining the data-splits and the confidence level threshold, we combined the training and validation dataset to re-train the network before splitting.</p><p>Quantitative Results: Our proposed method performance in comparison to the existing methods is provided in <ref type="table" target="#tab_0">Table 1</ref>. we obtain a test error of 9.68% without any data-augmentation and this sets a new state-of-the-art results on CIFAR-10 dataset. When compared to our strong baseline NIN (same model complexity) we improved the performance by nearly 1%.</p><p>Further Analysis: <ref type="figure">Figure 5</ref>(a) provides the confusion matrix of the root node at level-1. <ref type="figure">Figure 5(c)</ref> shows the clusters of confusion classes, obtained by applying a spectral co-clustering algorithm. We observe three clusters -Cluster-1: {0-airplane, 8-ship}, Cluster-2: {1-automobile, 9-truck}, Cluster-3: {2-bird, 3-cat, 4-deer, 5-dog, 6-frog, 7-horse}. This clustering can be interpreted as a data hier- <ref type="figure">Figure 4</ref>. Visualization of the learnt feature space on CIFAR-10 dataset. Each point corresponds to an image in CIFAR-10 dataset, and it's color correspond to its image class. Observe that the samples from certain classes gets cluster while others are separated. For instance, Class-1 (magenta) and Class-9 (green) belong to the same cluster, hence they are close to each other but away from the remaining classes. archy automatically generated from the data.</p><p>As described in Section 3.3, we use a joint-loss optimization to fine-tune the network which helps in block diagonalizing the confusion matrix. The impact of using the joint loss can be observed in <ref type="figure">Figure 4</ref>; notice that the use of joint loss brings the samples belonging to the same cluster closer while the samples in different clusters are moved farther apart in the feature space. We try to minimize the joint-loss function without compromising on the classification performance and in fact, this is shown to slightly improve the performance. <ref type="figure">Figure 5(b)</ref> shows the effect of varying the cluster size K on the error rate (due to misclassification); notice the error is least when K is 3. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Test Error Stochastic Pooling <ref type="bibr" target="#b32">[33]</ref> 15.13 CNN + Spearmint <ref type="bibr" target="#b24">[25]</ref> 14.98 Conv. maxout +Dropout <ref type="bibr" target="#b4">[5]</ref> 11.68 NIN+Dropout <ref type="bibr" target="#b14">[15]</ref> 10.41 DSN <ref type="bibr" target="#b13">[14]</ref> 9.78 DDN (ours) 9.68</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">CIFAR-100</head><p>Experimental Setup: The CIFAR-100 dataset <ref type="bibr" target="#b10">[11]</ref> consists of 100 classes of natural images, making it more challenging compared to the CIFAR-10 dataset. It consists of 50K training and 10K testing images. The number of training samples per class is only 100 as compared to 1000 in CIFAR-10. The dataset is pre-processed using global contrast normalization and ZCA whitening, as described in <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b14">15]</ref>. Similar to NIN <ref type="bibr" target="#b14">[15]</ref>, the last 10K samples of the training set were used as the validation dataset.</p><p>Quantitative Results: Our proposed method yields a test error of 31.65%, surpassing the current state-of-theart reported using Deeply Supervised Nets (DSN <ref type="bibr" target="#b13">[14]</ref>) by nearly 3%. The performance comparison is shown in Table 2. Note that HD-CNN <ref type="bibr" target="#b31">[32]</ref> uses data augmentation and 10 crop testing, so the performance is not directly comparable to other methods, since it is difficult to isolate the impact of the data augmentation from the methodology. However notice that DDN still performs better than HD-CNN even without any data augmentation.</p><p>We cannot directly compare with <ref type="bibr" target="#b25">[26]</ref> because they have reported numbers only with data augmentation. Since data augmentation process for CIFAR-100 is not standardized, we reported numbers without augmentation to enable a fair comparison with the existing literature and showed a significant performance improvement. <ref type="table">Table 2</ref>. Results on CIFAR-100 Dataset without data augmentation. *-with data augmentation and 10 view testing <ref type="bibr" target="#b11">[12]</ref> Method</p><p>Test Error Learned Pooling <ref type="bibr" target="#b16">[17]</ref> 43.71 stochastic Polling <ref type="bibr" target="#b32">[33]</ref> 42.51 Conv. maxout +Dropout <ref type="bibr" target="#b4">[5]</ref> 38.57 Tree based priors <ref type="bibr" target="#b27">[28]</ref> 36.85 NIN+Dropout <ref type="bibr" target="#b14">[15]</ref> 35.68 DSN <ref type="bibr" target="#b13">[14]</ref> 34.57 NIN+LA units <ref type="bibr" target="#b0">[1]</ref> 34.40 HD-CNN* (best) <ref type="bibr" target="#b31">[32]</ref> 32.62 DDN (ours) <ref type="bibr" target="#b30">31</ref>.65</p><p>Further Analysis: <ref type="figure">Figure 6</ref>(a) provides the confusion matrix of the root node at level-1 and the resulting clusters obtained after applying spectral co-clustering are shown in <ref type="figure">Figure 6</ref>(c). The effect of varying the cluster size K on the error rate is shown in the <ref type="figure">Figure 6</ref>(b). Notice that even though the error rate is lowest at K = 3, the algorithm chose K = 6 for optimal clustering based on the overall fitness measure. Out of these six clusters, three of them were composed of a single class for which no expert network is required (since the image class in already determined). The other three clusters have 72, 17 and 8 classes. The expert networks for each of these clusters reduce the classification error by more than 10%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Detailed analysis of DDN performance</head><p>In <ref type="table" target="#tab_1">Table 3</ref>, we provide a detailed performance analysis of DDN at each node in the network in comparison to the baseline NIN. We have also reported the results using NIN with joint loss optimization (NIN + JL). Though the main objective of joint-loss optimization was to reduce the confusion cases across the cluster samples, we get some improvement (a) Confusion matrix for level-1/root node (b) Effect of cluster size K on error rate (c) Spectral co-clustering at K = 3 <ref type="figure">Figure 5</ref>. CIFAR-10 results. The optimal clustering obtained using the fitness measure is at K = 3, which incidentally also corresponds to the lowest miss-classification error.</p><p>(a) Confusion matrix of level-1/root node (b) Effect of cluster size K on error rate (c) Spectral co-clustering with K=6 <ref type="figure">Figure 6</ref>. CIFAR-100 results. Notice that even though the miss-classification error is lowest at K = 3, the optimal clustering obtained using the fitness measure is at K = 6. This is consistent with our intuition that the fitness measure encourages partitioning into more clusters while keep the error low.) over the baseline on both the datasets. This is probably because the joint-loss optimization helped in regularization of the network. DDN helps in providing some insight into the data as to what are the classes that are hard to distinguish from others. For instance, if we consider CIFAR-10, the root node (NIN) produced three clusters of confusion classes. We can see that cluster-3 performance is low when compared to other clusters. The reason being that cluster-3 had 6 classes: cat, dog, deer, dog, frog and horse, all of them belong to the animal category and it is relatively hard to distinguish among them when compared to the automobile/truck in cluster-2. It is also important to note that the DDN helped to improve the performance in each of the clusters that led to the overall improvement. This also verifies the fact that expert network nodes were in fact helpful as compared to training one large network end-to-end.</p><p>For CIFAR-100 dataset, the performance improvement of DDN is significant when compared to our baseline NIN. Part of the reason is that, DDN seem to benefit more from having more number of classes and there remains large room for improvement on this particular dataset. The expert network nodes were introduced only to address the clusters with atleast 2 classes and hence clusters with one class wouldn't get any performance improvement. This indicates that DDN by design would not bring down the performance of any of the existing network used at the root but it only tries to improve the performance by addressing the most confusing cases. Clusters with atleast two classes get benefited from the expert network node which results in an overall improved performance. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We presented a generic framework to build an efficient deep learning network that significantly improves the performance. DDN is a tree-like structured network built with NIN as the root node and all the expert network branch nodes made up of mlpconv layer. DDN significantly improved over the current state-of-the-art results on publicly available datasets: CIFAR-10 and CIFAR-100. In addition, DDN also helped to provide some insight into the data by identifying the most confusing classes and their performances. DDN also benefits from making early decisions in the deep network to meet the real-time performance. The proposed approach can be applied to any classification problem.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Overview of the Deep Decision Network (DDN) framework. We observe N levels in the DDN tree structured network and at each level there could be K clusters of confusion classes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>DDN optimization with weighted contrastive-loss function along with Softmax-loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>DDN method idea validation on classification of digit '6' and '8' of MNIST dataset. left image indicates some of the confusion classes at the level-1 and the right one indicates some confusion cases at level-2. One could observe that some of the confusion cases of level-1 are resolved at level-2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>y if (Î sj=n = f sj=n (Î sj=n−1 )) &gt; T sj=n {i} where the above mentioned parameters are defined as follows: I: input image, y: predicted label, s j : different stages of the network and j ∈ 1 . . . n, n: number of stages, f (.): embedding function of the network layer,Î: output of a previous layer and T sj {i}: threshold of a class label i at stage s j .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 .</head><label>1</label><figDesc>Results on CIFAR-10 Dataset without data augmentation</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 3 .</head><label>3</label><figDesc>Detailed Quantitative Performance on CIFAR-10 and CIFAR-100 Dataset. NIN+JL: NIN network with joint-loss optimization. Error(%) Level-0 Level-1 Error (%) Error (%) Error(%) Level-0 Level-1 Error (%)</figDesc><table>CIFAR-10 
CIFAR-100 
NIN 
NIN+JL 
DDN 
NIN 
NIN+JL 
DDN 
Error(%) Cluster-1 
-
7.15 
1148 
767 
7.0 
-
37.97 
1802 
5093 
34.52 
Cluster-2 
-
5.50 
668 
1280 
4.8 
-
14.0 
102 
0 
14.0 
Cluster-3 
-
12.43 
1704 
4199 
12.2 
-
24.0 
88 
0 
24.0 
Cluster-4 
-
-
-
-
-
-
26.35 
548 
983 
23.35 
Cluster-5 
-
-
-
-
-
-
28.62 
213 
483 
26.25 
Cluster-6 
-
-
-
-
-
-
24.0 
89 
0 
24.0 
Overall 
10.41 
9.99 
9.68 
35.68 
34.73 
31.55 

</table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Agostinelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sadowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Baldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6830</idno>
		<title level="m">Learning activation functions to improve deep neural networks</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Pattern recognition and machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Co-clustering documents and words using bipartite spectral graph partitioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the seventh ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the seventh ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="269" to="274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1302.4389</idno>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Maxout networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.01852</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Caffe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5093</idno>
		<title level="m">Convolutional architecture for fast feature embedding</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep neural decision forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kontschieder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fiterau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Bulo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intl. Conf. on Computer Vision (ICCV)</title>
		<meeting><address><addrLine>Santiago, Chile</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
	<note>winner of the david marr prize 2015</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>F. Pereira, C. Burges, L. Bottou, and K. Weinberger</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2003" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<ptr target="http://yann.lecun.com/exdb/mnist/" />
		<title level="m">MNIST handwritten digit database</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.5185</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">Deeply-supervised nets. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Network in network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.4400</idno>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.4038</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning smooth pooling regions for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">24th British Machine Vision Conference</title>
		<imprint>
			<publisher>BMVA Press</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">3d object recognition with deep belief nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1339" to="1347" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1306.2795</idno>
		<title level="m">Recurrent convolutional neural networks for scene parsing</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Simplifying decision trees. International journal of man-machine studies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Quinlan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="221" to="234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The boosting approach to machine learning: An overview</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Nonlinear estimation and classification</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="149" to="171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep learning in neural networks: An overview</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="85" to="117" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.03832</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Practical bayesian optimization of machine learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2951" to="2959" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Rippel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Satish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sundaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Patwary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.05700</idno>
		<title level="m">Scalable bayesian optimization using deep neural networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Discriminative transfer learning with tree-based priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Going deeper with convolutions. CoRR, abs/1409.4842, 2014. 1, 2</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deeppose: Human pose estimation via deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1653" to="1660" />
		</imprint>
	</monogr>
	<note>IEEE</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Regularization of neural networks using dropconnect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">L</forename><surname>Cun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Machine Learning (ICML-13)</title>
		<meeting>the 30th International Conference on Machine Learning (ICML-13)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1058" to="1066" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Hd-cnn: Hierarchical deep convolutional neural network for large scale visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Piramuthu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jagadeesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>De-Coste</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015-12" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3557</idno>
		<title level="m">Stochastic pooling for regularization of deep convolutional neural networks</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
