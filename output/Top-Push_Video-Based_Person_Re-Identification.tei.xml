<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Top-push Video-based Person Re-identification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinjie</forename><surname>You</surname></persName>
							<email>youjinjie9@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Intelligence Science and System Lab</orgName>
								<orgName type="institution">Sun Yat-sen University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">Guangdong Provincial Key Laboratory of Computational Science</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ancong</forename><surname>Wu</surname></persName>
							<email>wuancong@mail2.sysu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Intelligence Science and System Lab</orgName>
								<orgName type="institution">Sun Yat-sen University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">Guangdong Provincial Key Laboratory of Computational Science</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
							<email>lixiang651@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Intelligence Science and System Lab</orgName>
								<orgName type="institution">Sun Yat-sen University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">Guangdong Provincial Key Laboratory of Computational Science</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Shi</forename><surname>Zheng</surname></persName>
							<email>wszheng@ieee.org</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Intelligence Science and System Lab</orgName>
								<orgName type="institution">Sun Yat-sen University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Data and Computer Science</orgName>
								<orgName type="institution">Sun Yat-sen University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">Guangdong Provincial Key Laboratory of Computational Science</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Top-push Video-based Person Re-identification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:47+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Most existing person re-identification (re-id) models focus on matching still person images across disjoint camera views. Since only limited information can be exploited from still images, it is hard (if not impossible) to overcome the occlusion, pose and camera-view change, and lighting variation problems. In comparison, video-based re-id methods can utilize extra space-time information, which contains much more rich cues for matching to overcome the mentioned problems. However, we find that when using video-based representation, some inter-class difference can be much more obscure than the one when using still-imagebased representation, because different people could not only have similar appearance but also have similar motions and actions which are hard to align. To solve this problem, we propose a top-push distance learning model (TDL), in which we integrate a top-push constrain for matching video features of persons. The top-push constraint enforces the optimization on top-rank matching in re-id, so as to make the matching model more effective towards selecting more discriminative features to distinguish different persons. Our experiments show that the proposed video-based reid framework outperforms the state-of-the-art video-based re-id methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Person re-identification (re-id) matches persons across non-overlapping camera views at different time. Most existing works focus on matching still images represented by appearance features (e.g. color histograms), because of computation efficiency and limited storage space. Given a probe image, we match it against a set of gallery images, which may suffer from illumination change, viewpoint difference, complicated background and occlusions. The significant visual ambiguity and appearance variation make still-image-based person re-id a challenging problem. Many methods have been developed to either extract invariant features or learn discriminative matching models <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b2">3]</ref>.</p><p>However, the still-image-based person re-id indicates that the temporal information between images of a person in each camera view is ignored. Information of a still image is sometimes not enough for recognizing a person, e.g. the person being occluded by objects or other persons (see Figure 1 for example). As surveillance information is recorded by videos and human operators always recognize persons in videos, it is intuitive to mine more effective information in video re-id. What more information can we obtain from videos than still images? Firstly, video is an image sequence containing space-time information, in which motion information is available. Secondly, appearance cues are more abundant in a sequence than in a still image, which can facilitate extracting more robust appearance features. Thirdly, occlusion and background influence can be eliminated to some extent. In a sequence, background variation and occlusion can be regarded as removable noises, while in still images they are troubling interferences.</p><p>Although more information can be obtained from per-  <ref type="figure">Figure 2</ref>. In (a), on each dataset, we show two video instances of different people, who are wearing similar clothes and walking similarly. In (b), we randomly selected 20 video instances of different people. For each instance, we compute its image frame level feature representation (color&amp;LBP) and video level's (HOG3D+Color&amp;LBP(pooling)) where for the image level's we randomly selected one image frame from a video instance. For each level's representation, we compute the largest intra-class distance Dw and the smallest inter-class distance D b with respect to each sample. The x-axis is the index of the 20 random samples and the y-axis is the value of Dw/D b . It can be observed that most ratio values of videos are larger than those of image frames, i.e. these videos have more ambiguities than images. son videos, more challenges come along. Firstly, like the still-image-based approaches, video-based person representations are also similar because of similar appearance. Secondly, although the motion of a pedestrian is a kind of behavioral biometrics, that is an important discriminative cue for identifying different persons, it is unfortunate that the walking actions or other motions of different persons may be similar as well (see <ref type="figure">Figure 2</ref> (a) for example), which means the inter-class variation may be smaller for videobased representation of a person. As shown in <ref type="figure">Figure 2</ref> (b), we demonstrate that for some instances, it is harder to distinguish the video representations of different identities (due to large Dw D b value) than the still image cases. It shows that the ratio between maximum intra-class distance and minimum inter-class distance is much larger for the video-based re-id as compared to the image-based re-id because some motion information of different people could be similar. This suggests the ambiguity of videos is more serious, and it is true that more intra-class distances are larger than the related minimum inter-class distance. The observation here would imply the discriminative information could be hidden in the minor difference of actions and motion. To mine these minor differences in the data, more stringent constraint should be exploited to look for a latent space to maximize the inter-class margin between different persons. So far, only a few video-based methods <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11]</ref> have been developed. However, the mentioned problem for video-based person re-id still remains unsolved.</p><p>To address the above problem in video-based person reid, we propose a top-push distance learning model (TDL) in this work. For a person video sequence, we exploit a feature representation constituted by HOG3D <ref type="bibr" target="#b11">[12]</ref> and the average pooling of color histograms and LBP features <ref type="bibr" target="#b8">[9]</ref>. Based on that, we propose a discriminative distance model optimized towards the realization of the top-push distance constraint combined with the minimization of intra-class variations. We employ the idea of top-push in <ref type="bibr" target="#b14">[15]</ref> and introduce it into distance metric learning, in order to optimize the matching accuracy at the top rank for person re-id, which helps to look for a latent feature space to explicitly enlarge the interclass margin between video sequences.</p><p>Extensive experiments have been conducted on two video datasets including PRID 2011 <ref type="bibr" target="#b7">[8]</ref> and iLIDS-VID <ref type="bibr" target="#b30">[31]</ref> to validate the effectiveness of the proposed TDL model. Our results demonstrate that (1) by formulating the video-based person re-id problem as a distance metric learning problem with top-push constraint modeling, significant improvement on matching accuracy can be obtained against the existing video-based person re-id techniques; and (2) our proposed TDL model outperforms not only related distance/rank learning methods but also related representative still-image-based person re-id methods applied for the video-based person re-id problem under multi-shot setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>The unsolved problem of person re-id caused by lighting change, viewpoint change, occlusions and intricate background has been increasingly focused on and becomes an important topic in visual surveillance in the last five years. To overcome these challenges, most of existing works can  be categorized into extracting discriminant/relible features <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b34">35]</ref> or learning robust metrics or subspaces for matching <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22]</ref> in recent works. However, all these works use appearance features of still images to match, which may suffer from small inter-class variations caused by similar pedestrians clothing and large intra-class variations caused by occlusions. Although it is natural to extend them to handle videobased person re-id under a multi-shot setting, it is not an optimal way as shown in our experiments, moreover it takes more times for matching due to the increase of gallery size.</p><p>Recently, a few works started to consider solving the person video matching problem in re-id. Dynamic Time Warping (DTW), which is a popular sequence matching method widely used for action recognition <ref type="bibr" target="#b22">[23]</ref>, was applied for video-based person re-id <ref type="bibr" target="#b29">[30]</ref>. Wang et al. <ref type="bibr" target="#b30">[31]</ref> introduced a pictorial video segmentation approach and deployed a fragment selecting and ranking model for person matching. Srikrishna et al. <ref type="bibr" target="#b9">[10]</ref> introduced a block sparse model to handle the video-based person re-id problem by the recovery problem on embedding space. However, these works assume all image sequences are synchronized, but it becomes unapplicable due to different actions taken by different people. It is also costly and difficult to obtain perfectly aligned pairwise person videos across non-overlapping camera views. All these works use either multiple images or a selected fragment of a sequence to extract feature, and thus they ignore the integrity and the richness of video features. So they are ineffective for solving the video-based person re-id problem.</p><p>We extend the use of top-push constraint from linear ranking function <ref type="bibr" target="#b14">[15]</ref> to second-order distance metric learning in our TDL model. Both the proposed TDL model and the linear function in <ref type="bibr" target="#b14">[15]</ref> aim to optimize the top-rank matching performance. The difference is that the TDL model is able to look for a latent subspace rather than computing only one ranking function score, so that more robust latent features can be exploited. Since, the top rank linear function learning is a RankSVM <ref type="bibr" target="#b28">[29]</ref> like learning, which has been shown very costly on high dimensional and moderately large-scale dataset <ref type="bibr" target="#b41">[42]</ref>, the top rank linear function learning cannot be straightforward generalized to a multiple dimensional one <ref type="bibr" target="#b14">[15]</ref>. Our experiments suggest that exploring a subspace rather than a hyperplane is more robust for person re-id.</p><p>Different from existing distance metric learning methods, our proposed TDL model is specially motivated from the observation that the inter-class variation is much smaller on video level than that on still image level, so the toppush constraint, a more effective relative comparison, is employed to explicitly avert this problem in a latent feature space. In particular, our approach is related to Weinberger et al.'s LMNN method <ref type="bibr" target="#b33">[34]</ref>. LMNN aims at optimizing KNN classification by using the local structure of the data. For each instance, a local neighborhood is established, including the k nearest neighbors sharing the same label (target neighbors). Samples that invade this perimeter with a different label (impostors) are penalized (see <ref type="figure" target="#fig_3">Figure 3</ref> (a)). Our method seems similar to LMNN; however, an important difference is that a more stringent top-push constraint is used to guide the distance learning, which notably benefits the top-rank matching results in person re-id (see <ref type="bibr">Figure 3 (b)</ref>). Ours is also related to the relative distance comparison (RDC) <ref type="bibr" target="#b40">[41]</ref>. While RDC is limited by the scale of relative comparison, the proposed TDL can largely reduce the number of relative comparisons in the context of top-push modeling. In addition, compared to LDA <ref type="bibr" target="#b4">[5]</ref>, our model replaces the maximum of inter-class distance by the minimization of hinge loss of top-push comparison, so that our model has imposed much more powerful constraint on the inter-class modeling. The significant improvement against LMNN, RDC and LDA will be shown in the experiment part.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach</head><p>The feature representation of a person video in our model has two main components: space-time features and appearance features. For extracting the space-time features, we employ the HOG3D descriptor <ref type="bibr" target="#b11">[12]</ref> to represent the person video. The HOG3D feature contains spatial gradient and temporal dynamic information. For extracting the appearance features, we first use color histograms and LBP features <ref type="bibr" target="#b8">[9]</ref> to describe a person appearance in each image frame. To obtain stable appearance cues and suppress noises caused by occlusions, we express the appearance features of a person video by average pooling of features of all frames from that video. The average pooling of color histograms and LBP features can represent rich appearance information of a person in video. The space-time features and the appearance features describe different information of a person in video, and those two types of features are complementary. Therefore in our model, the two features are combined to address the challenging video-based person re-id problem caused by background change, occlusions and motions.</p><p>We denote the training set by</p><formula xml:id="formula_0">X = {( x i ,y i )} s i=1 , where x i ∈ R d</formula><p>is the feature vector extracted from a video of person labeled y i . We denote the distance between any two feature vectors x i and x j by D( x i , x j ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Enhancing Top-rank Matching by Top-push Distance Learning</head><p>For person re-id, it is always expected that, for a query image, the top-rank matching of gallery images is correct. This means the distance between any matched gallery sample and the query should be smaller than the one between any unmatched one and the query. Therefore, in our distance metric learning modeling, we are concerning the relative comparison between the distance of a positive pair and the minimum distance of all related negative pairs, rather than comparing the positive pair with each of the related negative pair. In formulation, that is, for each example x i , we wish to realize the following comparison:</p><formula xml:id="formula_1">D( x i , x j )+ρ&lt; min y k =yi D( x i , x k ),y i = y j ,<label>(1)</label></formula><p>where ρ is a slack parameter. In this work, we set ρ =1.T o quantify the above comparison, we aim to minimize a hinge loss function incurred by the positive pairs whose distances are not smaller than the smallest distance of negative pairs with respect to input x i :</p><formula xml:id="formula_2">min xi, xj ,yi=yj max D( x i , x j ) − min y k =yi D( x i , x k )+ρ, 0 .</formula><p>(2) The minimization of the loss of the above comparison refers to inter-class separation, which however does not address the intra-class variation. Therefore, we also wish to strengthen the correlation of samples of any positive pair by minimizing the distance between samples of the same class in the meanwhile, i.e., min xi, xj ,yi=yj</p><formula xml:id="formula_3">D( x i , x j ).<label>(3)</label></formula><p>Therefore, the objective function of top-push distance learning is formulated below:</p><formula xml:id="formula_4">f (D)=(1− α) xi, xj ,yi=yj D( x i , x j ) +α xi, xj ,yi=yj max D( x i , x j ) − min y k =yi D( x i , x k )+ρ, 0 ,<label>(4)</label></formula><p>where α ∈ [0, 1] refers to a weighting parameter that balances the two terms. We call the second term the top-push  constraint. Through the optimization, the first term penalizes large distances between positive pairs, and meanwhile the second term penalizes small distances between each sample and the closest sample that is differently labeled. The learning induced by this cost function are illustrated in <ref type="figure" target="#fig_3">Figure 3</ref> for an input. We call our approach the top-push distance learning (TDL). In TDL, we specially consider the optimization of Mahalanobis distance under Criterion 4, i.e., considering</p><formula xml:id="formula_5">D( x i , x j )=( x i − x j ) ⊤ M( x i − x j ),<label>(5)</label></formula><p>where M 0 is a positive semi-definite matrix.</p><p>To visualize the effectiveness of TDL, a comparison between the data distributions of the original feature space and the latent feature space learned by TDL is shown in <ref type="figure" target="#fig_5">Figure 4</ref>. The change of distribution indicates that the input data samples of the same person are ambiguous, while T-DL does reduce the ambiguities and the data distribution is much favorable for classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Optimization</head><p>To simplify our notation, we denote the outer product of pairwise differences by</p><formula xml:id="formula_6">X i,j =( x i − x j )( x i − x j ) ⊤ .<label>(6)</label></formula><p>Based on Eq. (6), we can reformaulate D( x i , x j ) as follows:</p><formula xml:id="formula_7">D( x i , x j )=tr (MX i,j )<label>(7)</label></formula><p>and therefore we can reformulate the objective function Eq.(4) as:   </p><formula xml:id="formula_8">Gt = ∂f M | M=M t =(1 − α) i,j Xi,j + α (i,j,k)∈N (M t ) (Xi,j − X i,k ).<label>(9)</label></formula><p>The optimization of Eq.(8) must satisfy the constraint that the matrix M t+1 remains positive semi-definite. For this purpose, we project M t+1 onto the cone of all positive semi-definite matrices P + after each gradient descent step. To be specific, we first perform the eigen-decomposition on M t+1 :</p><formula xml:id="formula_9">M t+1 = V t+1 D t+1 V ⊤ t+1 .<label>(10)</label></formula><p>In order to apply the projection, we will update the diagonal matrix D t+1 by removing all the negative eigenvalues, and then reconstruct M t+1 by Eq.(10). The algorithm is summarized in Algorithm 1. We denote the gradient step size by λ&gt;0. In practice, it worked starting with λ =1 e − 03. Then, at each iteration, we increased λ by a factor of 1.01 if the loss function decreased and decreased λ by a factor of 0.5 if the loss function increased.</p><p>Matching. The learned metric can be exploited to perform person re-id by matching a probe person video sequence x p against a gallery set { x g } in another camera view. The distance between a probe video sequence x p and a gallery video sequence x g is computed by</p><formula xml:id="formula_10">D( x p , x g )=( x p − x g ) ⊤ M( x p − x g ).<label>(11)</label></formula><p>4. Experiments</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets and settings</head><p>Datasets. Our experiments were conducted on two publicly available video datasets for video-based person re-id: the PRID 2011 dataset <ref type="bibr" target="#b7">[8]</ref> and the iLIDS-VID dataset <ref type="bibr" target="#b30">[31]</ref>. Settings. In our experiments, we adopted a single-shot experiment setting. All datasets were randomly divided into training set and testing set by half so that there were p =89and p = 150 individuals in the testing sets of PRID 2011 and iLIDS-VID respectively. In the testing stage, the videos from one camera were used as the gallery set while the ones from another camera as the probe set. The cumulative matching characteristic (CMC) curve is used to measure the performance of each method on each dataset. A rank k matching rate indicates the accuracy of the matching between the probe video x p and the gallery videos</p><formula xml:id="formula_11">{ x g } k g=1</formula><p>in the top k rank list. To obtain statistically reliable results, we repeated the procedure 10 times and reported the average results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Feature Extraction</head><p>To obtain more abundant and robust features for representing a person video, we explored a combined person video feature representation. We expressed each sample with appearance feature on image frame level and spacetime feature on video level. Specifically, at the image frame level, each frame of the person video was resized to 128×48  pixels and divided into patches with size 8 × 16 with 50% overlap both in the horizontal and vertical directions. That is to say, there were 155 patches for extracting color histograms and LBP features <ref type="bibr" target="#b8">[9]</ref>. For each patch, histograms of color channels in HSV and LAB color spaces and LBP descriptor were computed. All the appearance feature descriptors within the image frame were concatenated together to form a 1705-dimensional feature vector. At the video level, we extracted a 1200-dimensional HOG3D feature vector for each person video <ref type="bibr" target="#b11">[12]</ref>. In the end, we described the whole person video using a 2905-dimensional vector by connecting this HOG3D feature with average pooling of color histograms and LBP features over all image frames of the video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Evaluation of Comparison</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Comparison with the State-of-the-art Methods</head><p>In <ref type="table">Table 1</ref>, we reported the comparison of our proposed TDL model with the existing six state-of-the-art videobased person re-id methods on PRID 2011 and iLIDS-VID datasets, including SDALF <ref type="bibr" target="#b3">[4]</ref>, Salience <ref type="bibr" target="#b37">[38]</ref>, RPRF <ref type="bibr" target="#b18">[19]</ref>, SRID <ref type="bibr" target="#b9">[10]</ref>, DVDL <ref type="bibr" target="#b10">[11]</ref> and Color&amp;LBP+DVR <ref type="bibr" target="#b30">[31]</ref>. D-VDL is a dictionary learning method based on multi-shot re-id datasets. DVR is a method based on ranking model, which also selects discriminative video fragment from a candidates pool in the training process. The results show clearly that with the proposed TDL model, the matching performance on both datasets is improved significantly. For instance, on iLIDS-VID dataset, our TDL improved the Rank-1 matching rate by 21.8% compared to the second best method Color&amp;LBP+DVR.</p><p>Another interesting but indeed fact can be observed is that TDL outperformed others much better on iLIDS-VID. We examined that this is probably because more intra-class distances could be much larger than inter-class ones under more occlusions on iLIDS-VID. While the compared distance models do not explicitly and directly quantify the relation between each intra-class distance and the related minimum inter-class distance, the proposed TDL employs the top-push strategy and makes the distance model quantify more effective features and thus performs more stably.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Comparison with Related Methods</head><p>There are several existing distance/subspace learning models usually applied for person re-id. For fair comparison, all compared methods used the same feature representation of person videos described in Sec. 4.2. We first compared our TDL with representative rank/distance/subspace learning methods for video-based matching, e.g. TopRank <ref type="bibr" target="#b14">[15]</ref>, linear discriminant analysis (LDA) <ref type="bibr" target="#b4">[5]</ref> and LMNN <ref type="bibr" target="#b33">[34]</ref>. Our results ( <ref type="figure" target="#fig_11">Figure 6</ref> and <ref type="table">Table 2</ref>) show clearly that the proposed TDL model obtains better matching rates than these methods. More specifically, on PRID 2011 dataset, the Rank-1 matching rate is 56.74% for TDL, whilst 31.69% for TopRank, 15.84% for LDA, and 27.19% for LMNN. These results show that these related methods performed poorly for video-based person re-id. As seen from the comparison video-based matching results in <ref type="figure" target="#fig_11">Figure 6</ref> and <ref type="table">Table 2</ref>, the improvement was particularly significant on iLIDS-VID dataset, which is more challenging due to more ambiguities caused by occlusions and illumination. With the top-push constraint, the ambiguities can be better removed.</p><p>The video-based matching results of several representa-  tive still-image-based person re-id methods are also shown in <ref type="figure" target="#fig_11">Figure 6</ref> and <ref type="table">Table 2</ref>, including L1-norm, LFDA <ref type="bibr" target="#b27">[28]</ref>, KISSME <ref type="bibr" target="#b12">[13]</ref>, LADF <ref type="bibr" target="#b19">[20]</ref>, PRSVM <ref type="bibr" target="#b28">[29]</ref>, RDC <ref type="bibr" target="#b40">[41]</ref> and ISR <ref type="bibr" target="#b23">[24]</ref>. One can observe that our TDL model always outperformed all the compared re-id methods on both datasets. The improvement is particularly significant on iLIDS-VID dataset, and TDL is 17.33% higher than the best compared the method at Rank-1. In addition, among these six re-id methods, RDC is closely related to our model, but RDC is limited by the scale of relative comparison. In our experiments, the computational cost of our model was only 3% of the one of RDC. These results highlight the effectiveness of the proposed model.</p><p>One may wonder when using multiple image frames, whether existing still-image-based methods can achieve better performance. To answer the question, in this section, we adopted a multi-frame setting to conduct the experiments, in which 5 images of each person were randomly selected from all frames as gallery. We used the combined appearance features (Color&amp;LBP&amp;HOG) <ref type="bibr" target="#b17">[18]</ref> as representation of still image frames. We performed experiments on the two datasets and the results are also reported in <ref type="table">Table 2</ref>.</p><p>Since RDC, PRSVM and TopRank suffered from the huge computational cost with increasing size of training set under multi-frame or multi-shot setting, these methods cannot be run on a server with 64GB RAM. To be more specifical, when conducting iLIDS-VID (consists of 300 persons) under multi-shot setting, not just more persons were involved but also more images were used (10 frames for each person in the training set), so that the number of triplets for relative comparison increases dramatically (more than 10 8 ). RDC and PRSVM are designed to utilize all the triplets for training, and it is clear that RDC and PRSVM are costly and not computational trackable.</p><p>Compared to the video-based matching results, it is evident that all the still-image-based methods performed poorly, worse than their video-feature-based versions. This suggests that space-time video information is an important cue to augment the feature representation for person re-id; that is, video-based matching is more effective than multiple image frames matching.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Further Evaluation of TDL</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">Effects of Different Feature Components</head><p>The feature representation used in our proposed model consists of two components: space-time features (HOG3D) and appearance features (Color&amp;LBP (pooling)). In <ref type="figure" target="#fig_12">Figure 7</ref> , we evaluated the effects of each component respectively. The results show that all of them are effective on their own, and when they are combined, the best performance is achieved. This validates that these feature components are complementary and should be fused.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">Influence of Parameters</head><p>We implemented our TDL model by selecting the parameter α on PRID 2011 and iLIDS-VID datasets. The results of area under CMC curve (AUC) were plotted in <ref type="figure" target="#fig_7">Figure 8</ref> (a) and (b). As illustrated, when α was around 0.1, the model achieved the best result. The figures suggest the performance of using and not using top-push constraint in TDL is distinct. When it is not integrated, the optimization problem Eq. (8) becomes trivial since M = O where O is a zero matrix is the optimal solution which cannot be effective for classification. We also observe that when α =1 , i.e., discarding the intra-class variation minimization, it will also lead to overfitting in top-push. Thus a proper α, for instance 0.1 here is a balance. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work, we have proposed a top-push distance learning (TDL) model to address the video-based person re-identification problem. While video-based representation contains more abundant space-time information than still-image based representation, there are more ambiguities in video-based features than still-image-based features. So we introduce a top-push constraint to quantify ambiguous video representation. Due to the employment of top-push constraint, the formed distance model can be more effective on top-rank performance of video-based person re-id. This is validated on through extensive experiments conducted on two video datasets including PRID 2011 and iLIDS-VID.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Video instances vs. still-image instances of the same person. It is clear that video contains much richer cues for matching.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>LMNN (left) vs. our TDL (right). Compared to LMNN, since the minimum inter-class distance is considered in TDL, the imposters are more heavily penalized.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>Illustration of the effectiveness of TDL, where 10 different persons in PRID 2011 dataset were selected for demonstration. Points of different colors indicate different persons. The left are the person data points in the original 2-D space and the right is the projected person data points in the 2-D space learned by TDL. The projection matrix L is obtained by decomposing matrix M into M = L ⊤ L.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>xj ,yi=yj tr (MX i,j ) +α xi, xj ,yi=yj max tr (MX i,j ) − min y k =yi tr (MX i,k )+ρ, 0 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>( 8 )</head><label>8</label><figDesc>Algorithm 1 The Optimisation Algorithm for TDL.Initialize:Initialize metric with the identity matrix M 0 := I;The triggered set N (M 0 ):={} ; The gradient G t := (1 − α) i,j X i,j ;The counter t := 0. 1: while (not converged) triggered set N (M t ) by indices (i, j, k) determined by the second term of Eq.t+1 onto the cone of all positive semidefinite matrices P + (M t+1 ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>= t +1. 8: end while 9: return M t . Our model applies a stochastic gradient descent projection method to compute an optimized positive semi-definite matrix M in Eq.(8). In particular, at the t-th iteration, Eq.(8) is piecewise linear with respect to M. At step t,given M = M t , we define a set of indices (i, j, k) ∈N (M t ), if and only if the indices (i, j, k) trigger the second term of Eq.(8). The stochastic gradient G t of f (M) at step t is computed by:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>The PRID 2011 dataset consists of video pairs recorded from two different but static surveillance cameras. 385 persons were recorded in camera view A, and 749 persons in camera view B. Among all persons, 200 persons were recorded in both camera views. Each video is comprised of 5 to 675 image frames, with an average of 100 for each. To guarantee the effective length of the video, we selected 178 persons with more than 27 frames in our experiments. This dataset was captured in uncrowded outdoor scenes with relatively simple and clean background and rare occlusions, and several different poses of person are available in each camera view (Figure 5(a)). The iLIDS-VID dataset contains 600 video of 300 randomly sampled people. Each person has one pair of video from two camera views. Each video is comprised of 23 to 192 image frames, with an average of 73 for each. Compared with the PRID 2011 dataset, it was captured in an airport arrival hall under a multi-camera CCTV network. The challenges of this dataset largely lie in clothing similarities, lighting and viewpoint changes across camera views, complicated background and occlusions(Figure 5(b)).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 5 .</head><label>5</label><figDesc>Example pairs of image sequences of the same person appearing in different camera views.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 6 .</head><label>6</label><figDesc>Video-based matching rates (%) of different methods on PRID 2011 and iLIDS-VID.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 7 .</head><label>7</label><figDesc>Evaluation of different feature components in TDL on PRID 2011 and iLIDS-VID. The rank 1 matching rate of each method is provided in the legend.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 8 .</head><label>8</label><figDesc>Parameter sensitivity analysis on PRID 2011 and iLIDS-VID</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>Table 2. Comparison with related methods on PRID 2011 and iLIDS-VID datasets. The matching rate (%) at Rank i means the accuracy of the matching within the top i gallery classes.</figDesc><table>Settings 
Methods 
PRID 2011 
iLIDS-VID 
Rank 
Rank-1 
Rank-5 
Rank-10 
Rank-20 
Rank-1 
Rank-5 
Rank-10 
Rank-20 
TDL 
56.74 
80.00 
87.64 
93.59 
56.33 
87.60 
95.60 
98.27 
L1-norm 
15.84 
30.00 
39.33 
52.70 
8.90 
21.40 
30.07 
42.07 
LFDA [28] 
43.70 
72.80 
81.69 
90.89 
32.93 
68.47 
82.20 
92.60 
KISSME [13] 
34.38 
61.68 
72.13 
81.01 
36.53 
67.80 
78.80 
87.07 
Video-based 
LADF [20] 
47.30 
75.50 
82.69 
91.12 
39.00 
76.80 
89.00 
96.80 
matching 
RDC [41] 
25.62 
47.30 
56.07 
74.38 
15.80 
36.93 
52.60 
66.00 
PRSVM [29] 
36.97 
60.45 
72.47 
83.03 
21.53 
50.60 
66.00 
80.80 
ISR [24] 
17.64 
35.84 
43.03 
54.38 
11.60 
22.13 
27.40 
36.67 
TopRank [15] 
31.69 
62.24 
75.28 
89.44 
22.53 
56.13 
72.73 
85.93 
LDA [5] 
15.84 
41.46 
55.51 
70.67 
42.06 
79.13 
89.40 
94.47 
LMNN [34] 
27.19 
53.71 
64.94 
75.17 
28.33 
61.40 
76.47 
88.93 

TDL 
30.22 
59.10 
74.04 
88.43 
9.81 
27.52 
46.10 
62.19 
L1-norm 
12.36 
29.44 
40.56 
56.40 
3.67 
10.33 
16.03 
26.93 
LFDA [28] 
26.40 
56.07 
69.89 
81.12 
7.80 
23.93 
36.47 
50.80 
Multiple image 
KISSME [13] 
28.54 
59.78 
72.13 
83.26 
10.67 
28.33 
39.80 
57.00 
frames matching 
LADF [20] 
8.20 
20.45 
29.89 
42.25 
4.33 
14.00 
21.20 
32.13 
ISR [24] 
10.50 
20.83 
31.83 
44.17 
8.04 
20.50 
31.33 
43.50 
LDA [5] 
27.64 
58.09 
69.66 
82.47 
10.27 
27.40 
39.80 
55.27 
LMNN [34] 
14.38 
38.09 
50.22 
67.19 
4.47 
13.20 
21.60 
35.47 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">An improved deep learning architecture for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">K</forename><surname>Marks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Mirror representation for modeling view-specific transform in person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">An asymmetric distance model for cross-view feature mapping in person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yuen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TCSVT</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Person re-identification by symmetry-driven accumulation of local features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Farenzena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bazzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Perina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Murino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cristani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Introduction to statistical pattern recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fukunaga</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>Academic press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Person reidentification using spatiotemporal appearance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gheissari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sebastian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hartley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Viewpoint invariant pedestrian recognition with an ensemble of localized features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Person re-identification by descriptive and discriminative classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hirzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Beleznai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Image Analysis</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Relaxed pairwise learned metric for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hirzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Köstinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Sparse re-id: Block sparsity for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karanam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Radke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshop</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Person re-identification with discriminatively trained viewpoint invariant dictionaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karanam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Radke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A spatio-temporal descriptor based on 3d-gradients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Klaser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marszałek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Large scale metric learning from equivalence constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Koestinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hirzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Color invariants for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kviatkovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rivlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Top rank optimization in linear time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-H</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Locally aligned feature transforms across views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deepreid: Deep filter pairing neural network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multiscale learning for low-resolution person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multi-shot re-identification with random-projection-based random forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Radke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning locally-adaptive decision functions for person verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Person re-identification by local maximal occurrence representation and metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Efficient psd constrained asymmetric metric learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Recognizing actions by shape-motion prototype trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Person re-identification by iterative re-weighted sparse ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lisanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Masi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bagdanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Del</forename><surname>Bimbo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1629" to="1642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Local descriptors encoded by fisher vectors for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jurie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Person re-identification over camera networks using multi-task distance metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3656" to="3670" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Pcca: A new approach for distance learning from sparse pairwise constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mignon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jurie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Local fisher discriminant analysis for pedestrian re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pedagadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Orwell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Velastin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Boghossian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Person re-identification by support vector ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Prosser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Mary</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Re-identification of pedestrians in crowds using dynamic time warping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Simonnet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lewandowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Velastin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Orwell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Turkbeyler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Person reidentification by video ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Shape and appearance context modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Doretto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sebastian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rittscher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Cross-scenario transfer person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TCSVT</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Distance metric learning for large margin nearest neighbor classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">K</forename><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="207" to="244" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">An enhanced deep feature representation for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Viewpoint invariant human re-identification in camera networks using pose priors and subject-discriminative features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Radke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Person reidentification using kernel-based metric learning methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Camps</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sznaier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Unsupervised salience learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning mid-level filters for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Person re-identification by probabilistic relative distance comparison</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Reidentification by relative distance comparison</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="653" to="668" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Towards open-world person re-identification by one-shot group-based verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Partial person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
