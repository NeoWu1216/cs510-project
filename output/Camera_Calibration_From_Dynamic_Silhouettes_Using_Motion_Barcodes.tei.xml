<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Camera Calibration from Dynamic Silhouettes Using Motion Barcodes</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gil</forename><surname>Ben-Artzi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">The Hebrew University of Jerusalem</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoni</forename><surname>Kasten</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">The Hebrew University of Jerusalem</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shmuel</forename><surname>Peleg</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">The Hebrew University of Jerusalem</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Werman</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">The Hebrew University of Jerusalem</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Camera Calibration from Dynamic Silhouettes Using Motion Barcodes</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:38+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Computing the epipolar geometry between cameras with very different viewpoints is often problematic as matching points are hard to find. In these cases, it has been proposed to use information from dynamic objects in the scene for suggesting point and line correspondences.</p><p>We propose a speed up of about two orders of magnitude, as well as an increase in robustness and accuracy, to methods computing epipolar geometry from dynamic silhouettes. This improvement is based on a new temporal signature: motion barcode for lines. Motion barcode is a binary temporal sequence for lines, indicating for each frame the existence of at least one foreground pixel on that line. The motion barcodes of two corresponding epipolar lines are very similar, so the search for corresponding epipolar lines can be limited only to lines having similar barcodes. The use of motion barcodes leads to increased speed, accuracy, and robustness in computing the epipolar geometry.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Calibration of multi-camera systems is normally computed by finding corresponding feature points between images taken by these cameras. When not enough feature points can be found, e.g. when the camera viewpoints vary greatly, the epipolar geometry can be computed from silhouettes of moving objects that are visible in videos captured by the two cameras. The silhouettes at one time instance are used to suggest matching epipolar lines which are used to propose a fundamental matrix that is verified over all frames.</p><p>The best methods for computing the fundamental matrix use tangents to the dynamic silhouette as candidates for epipolar lines <ref type="bibr" target="#b26">[27]</ref>. Our approach presents a speedup of about two orders of magnitude for these methods, and significantly improves accuracy and robustness. This speedup is obtained by requiring candidates for matching epipolar lines to share a temporal signature (motion barcode).</p><p>Motion barcodes were first introduced for points in <ref type="bibr" target="#b4">[5]</ref>. The motion barcode of a line is a binary temporal sequence, <ref type="bibr">Figure 1</ref>. When two cameras have very different viewpoints as in this example, appearance can not be used for calibration. Instead, calibration is possible from matching pairs of epipolar lines that can be extracted efficiently from moving silhouettes. The yellow lines are the epipolar lines proposed by our method, while the red lines are the ground truth epipolar lines. The corresponding silhouettes are displayed at the bottom.</p><p>indicating for each frame the existence of at least one foreground pixel on that line. We show that correlation between the motion barcodes of corresponding epipolar lines is high. By testing as possible matches only pairs of lines whose motion barcode correlation is high, a speedup by about two orders of magnitude is obtained. <ref type="figure">Figure 1</ref> shows matching epipolar lines extracted using our approach. Following <ref type="bibr" target="#b26">[27]</ref>, we use a RANSAC approach to test possible matching pairs of epipolar lines, and compute the epipolar geometry. This paper is organized as follows. Section 1.1 describes relevant prior work. Section 2 introduces the theoretical background. Section 3 presents the motion barcodes of lines. Section 4 shows how to match epipolar lines based on dynamic silhouettes. Section 5 presents an iterative computation of the fundamental matrix based on the motion barcode. Section 6 shows our results on both synthetic and real sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Prior Work</head><p>Extracting geometrical information from the motion of silhouettes include shape-from-silhouettes <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b2">3]</ref> and camera calibration <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b32">33]</ref>. In shape-from silhouettes, the goal is to recover the visual hull <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b22">23]</ref> of the object. If the cameras are calibrated, this task is relatively clear as each individual viewing cone <ref type="bibr" target="#b14">[15]</ref> can be backprojected and the visual hull is the intersection of these cones.</p><p>The case of uncalibrated cameras has also been investigated, where the goal is to recover epipolar geometry. The first step is to establish correspondences between special points on the silhouettes boundaries, called frontier points <ref type="bibr" target="#b8">[9]</ref>, across the different views. These points are images of object points that are tangent to an epipolar plane. Given corresponding frontier points, spatial constraints resulting from matching epipolar tangents <ref type="bibr" target="#b25">[26]</ref> are used to recover the epipolar geometry.</p><p>Matching corresponding frontier points and silhouette tangents can be found using robust estimation procedures such as RANSAC <ref type="bibr" target="#b12">[13]</ref>. Matching frontier points, or directions of four epipolar tangent lines, are initially guessed. Furukawa et al. <ref type="bibr" target="#b15">[16]</ref>, assuming orthographic projection, match frontier points using RANSAC. They used the distances between parallel tangent lines on the silhouettes as a geometric measure for matching. Given the epipoles and the accurate tangent envelope of the silhouettes, frontier points can be easily matched using two outermost epipolar tangents. This property was deployed by Wong and Cipolla <ref type="bibr" target="#b29">[30]</ref> for turntable motion. The most relevant previous work is Sinha and Pollefeys <ref type="bibr" target="#b26">[27]</ref>, addressing projective projection. They propose a RANSAC based search of possible epipoles, where a proposed epipole in each of the two corresponding images is generated from the intersection of two lines randomly selected from the tangent envelope.</p><p>Calibration without explicitly matching tangent epipolar lines has also been considered. <ref type="bibr" target="#b5">[6]</ref> used constraints based on the back projection of silhouettes boundaries in multiple views. <ref type="bibr" target="#b31">[32]</ref> jointly optimized the 3D position of frontier points and the camera parameters in a bundle adjustment. However, both methods require a good initialization of silhouette boundaries and camera parameters. Hernandez <ref type="bibr" target="#b17">[18]</ref> also proposed constraints based on the back projection of silhouettes for maximizing silhouette coherence, but his method is limited to turntable motion.</p><p>Binary temporal signatures of pixels which are based on the motion of the objects in the scene have been previously introduced. Ermis et al. <ref type="bibr" target="#b11">[12]</ref> deploy such features to find accurate correspondence between pixels across distributed cameras with the assumption of a distant, almost planar, scene. Drouin et al. <ref type="bibr" target="#b10">[11]</ref> matched 2D points between a video projector and a digital camera. They require a planar surface and the same ordering of pixels across views. Ben- <ref type="figure">Figure 2</ref>. The geometry of two views with a tangent epipolar plane and a frontier point. Π is the tangent epipolar plane and l, l ′ are tangent epipolar lines in Π. The epipolar plane is tangent to the object at the frontier points P .</p><p>Artzi et al. <ref type="bibr" target="#b4">[5]</ref> introduce a method to match events across different views even in the case of significant parallax and occlusions. However, that approach can not be applied to match pixels for camera calibration, as its localization is very inaccurate, as explained in Section 3. Using a temporal histogram as a temporal feature of a pixel is introduced in <ref type="bibr" target="#b18">[19]</ref>, but it is effective only for objects that are static for substantial periods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Theoretical Background</head><p>The geometric relation between corresponding silhouettes across views is based on frontier points and epipolar tangencies <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b20">21]</ref>.</p><p>The geometry of two views containing silhouettes is presented in <ref type="figure">Fig. 2</ref>. For the rest of the paper let candidate points be image points that are on the boundary of the silhouette as well as on the boundary of its convex hull. C and C ′ are the contours of the object in 3D. These contours project to silhouette boundaries S and S ′ . The two contours intersect in the frontier point P . The projections of P onto the two views are the candidate points, p and p'. The 3 points P, p, p ′ span a tangent epipolar plane Π between the two views. The points p and p ′ must lie on the corresponding tangent lines l and l ′ and the point P is the location where the tangent epipolar plane Π is tangent to the surface, e, e ′ are the epipoles. The frontier points are the only true corresponding points between the boundaries S and S ′ . If we have accurate tangents to the silhouettes and the location of the epipole is known, then the epipolar tangent lines give the corresponding points p and p ′ . This idea was traditionally used as a spatial cost function between corresponding tangential epipolar lines and points. See for example <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b26">27]</ref>.</p><p>Finding frontier points without the epipole locations is difficult <ref type="bibr" target="#b23">[24]</ref>. For a video sequence, when the location of at  least two frontier points are known, their tangent lines are epipolar lines. They can be used to calculate the epipole and the location of the other frontier points in all the other frames. Alternatively, if the epipoles are known we can use the tangent lines to the silhouettes to locate the frontier points. It follows that either the frontier points or the epipoles are needed in order to extract the epipolar lines.</p><p>Here we introduce a different approach which does not require prior knowledge of either frontier points or epipoles in order to extract matching epipolar lines. We directly compute epipolar line correspondences using the motion observed simultaneously by them. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Motion Barcode: Temporal Signature of Lines</head><p>Given two frames captured at the same time from different viewpoints, two corresponding pixels view a single 3D point. However, in a dynamic scene, a pixel in one view is bound to correspond to different pixels of the other view at different times, located on the corresponding epipolar line. <ref type="figure" target="#fig_0">Fig. 3</ref> illustrates a typical case. At time t = 1, a single pixel in the right view corresponds to some pixel in the left view. At time t = 2, due to the motion of the object in scene, the same pixel corresponds to a different pixel in the other view. For video sequences captured by stationary cameras the corresponding pixels will always reside on corresponding epipolar lines.</p><p>It follows that if an epipolar line contains at least one silhouette pixel at time t, then its corresponding epipolar line should contain such a pixel at the same time. This is illustrated in <ref type="figure" target="#fig_1">Fig. 4</ref>. For time t = 2, 3 there are points from objects that project onto the corresponding epipolar line. If a point on line l is part of a silhouette, this point or another silhouette point occluding it, will be seen on line l ′ . Alternatively, the silhouette could be blocked by a background object or be out of the frame.</p><p>The motion barcode of a line l, b l (t), indicates for each line l in frame t the existence of at least one foreground pixel on that line. b l (t) = 1 if the line intersects a silhouette, and b l (t) = 0 otherwise. The motion barcodes of two corresponding epipolar lines is very similar. Differences occur only in cases of occlusions.</p><p>The temporal similarity between two lines l and l ′ is defined as the correlation between their motion barcodes;</p><formula xml:id="formula_0">d t (l, l ′ ) = corr(b l (t), b l ′ (t))<label>(1)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Epipolar Geometry by Matching Lines</head><p>The epipolar geometry can be computed from 3 pairs of corresponding epipolar lines <ref type="bibr" target="#b16">[17]</ref>. The search space for matching epipolar lines across views is very large if we consider all possible pairs of lines. This search can be reduced to fewer lines by using only candidate lines, lines tangent to the tangent envelope of the silhouette. The tangent envelope includes points that are on the silhouette boundary as well as on the boundary of its convex hull. We follow the work of <ref type="bibr" target="#b26">[27]</ref>, checking for possible correspondence only lines on the tangent envelope. Using only candidate lines is justified as the projection of the frontier point is on the tangent envelope.</p><p>We select several corresponding pairs of frames from the video sequences, so that the pairs will be sufficiently different from each other. For each pair of frames, we sample K candidate lines from the tangent envelope of its silhouettes. We compute the correlation between the motion barcodes for all pairs of candidate lines from the two corresponding images. This results in K 2 correlations per each pair of frames. From every pair of frames we select the single pair of epipolar lines with the highest barcode correlation. <ref type="figure" target="#fig_2">Fig. 5</ref> shows all candidate lines, and <ref type="figure" target="#fig_3">Fig. 6</ref> shows the pairs of candidate lines having highest barcode correlation. We compute the epipolar geometry from three matching pairs based on <ref type="bibr" target="#b16">[17]</ref>. The computation is by RANSAC similarly to <ref type="bibr" target="#b26">[27]</ref>. The fundamental matrix is then fully optimized as described in Section 5.</p><p>The matching is carried out in two phases. An offline phase where the motion barcodes of the tangent lines are computed. In the online phase, the actual matching is carried out by computing the correlation between motion barcodes of pairs of lines, and computing the epipolar geometry using RANSAC. The overall efficiency depends mainly on accuracy of candidate matches as it effects the number of required iterations in the RANSAC phase. Details are in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Temporal Optimization of the Fundamental Matrix</head><p>Existing optimization techniques for computing the fundamental matrix are based on minimizing a spatial cost function without taking into account the temporal dimension. We present a technique based on both spatial and tem-poral cost functions (Eq. 1).</p><p>We assume a set of corresponding points, presumably the projection of frontier points</p><formula xml:id="formula_1">{(x i , x ′ i )} M i=1 , a set of cor- responding epipolar lines {(l i , l ′ i )} N i=1</formula><p>and an initial estimation of the fundamental matrix F . The optimization is iterative. In the first step we optimize the point correspondences based on the lines, using the geometric reprojection error <ref type="bibr" target="#b16">[17]</ref> as the spatial cost function. In the second step we optimize the epipolar line correspondences based on the given points, using the temporal cost function (Eq. 1). We optimize the directions of the epipolar lines for each pair of corresponding points. Based on the lines matched in this step, we estimate epipoles and an epipolar line homography. We then evaluate a set of corresponding points and obtain an estimation of the fundamental matrix. The process is described in the following:</p><p>• Step one:</p><formula xml:id="formula_2">1. Minimize reprojection error based on {(x i , x ′ i )} M i=1 . i d(x i ,x i ) 2 + d(x ′ i ,x ′ i ) 2 s.t.x iFxi ′ = 0</formula><p>This minimization is by the Levenberg-Marquardt procedure and gives a new set of points and fundamental matrix.</p><p>2. set l i =Fx i ′ , l ′ i =F Tx i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>Step two:</p><p>1. For each pair of lines, minimize</p><formula xml:id="formula_3">C l (l,l ′ ) = d s (l,l) + d s (l ′ ,l ′ ) − d t (l,l ′ )<label>(2)</label></formula><p>d s measures the angular deviation between lines, and d t is the barcode correlation (Eq. 1). d s ensures the lines are within an angle difference of no more than Θ. The choice of Θ will be discussed next.l,l ′ are sampled uniformly from [−Θ, Θ] around l, l ′ . We take the maximal match and if we have more than one maximum, the one with the minimal angle difference is selected.</p><p>2. Estimate new epipoles e, e ′ and epipolar line ho-</p><formula xml:id="formula_4">mography from {l i ,l ′ i }. 3. Set {x i , x ′</formula><p>i } by projecting onto the nearest l, l ′ . Estimate F from epipoles and lines homography.</p><p>The process terminates when the deviation of the estimated epipoles is small enough or a maximum number of iterations is exceeded.</p><p>The choice of the angular tolerance Θ defines the region where we look for the newly estimated lines. It depends on the epipolar envelope and the required probability for locating the line <ref type="bibr" target="#b28">[29]</ref>. Direct modeling of epipolar envelope is difficult and therefore it is empirically evaluated, see <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b16">17]</ref>. In our implementation we set Θ to 0.2 • which results in an accurate estimation. This reflects our assumption that the distortion is low. The specific choice can be adjusted according to the needs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experiments</head><p>Our approach was validated on synthetic and real sequences. We compared our method with the state of the art method <ref type="bibr" target="#b26">[27]</ref>, where the fundamental matrix is computed by RANSAC-based sampling of epipolar lines. The evaluation was done with the following datasets: the Kung-Fu girl <ref type="bibr" target="#b1">[2]</ref>, Boxer <ref type="bibr" target="#b3">[4]</ref>, Street Dancer <ref type="bibr" target="#b27">[28]</ref> and Dancing Girl <ref type="bibr" target="#b0">[1]</ref>. <ref type="figure" target="#fig_4">Fig. 7</ref> shows images from the datasets and <ref type="table">Table 1</ref> gives the details.</p><p>We compared the accuracy and efficiency of the two methods. The accuracy of the fundamental matrix in all experiments is measured by the symmetric epipolar distance (error) <ref type="bibr" target="#b16">[17]</ref> using ground truth matching points. The symmetric epipolar distance is the distance between each point and the epipolar line corresponding to the other point. The acquisition of the ground truth points is discussed in Subsection 6.3.</p><p>The efficiency of the methods is evaluated as follows. In both methods the fundamental matrix is computed using RANSAC sampling of epipolar lines. In each iteration, the symmetric epipolar distance of each hypothesis is evaluated. Every 1000 RANSAC iterations the best hypothesis  <ref type="table">Table 2</ref>. The expected number of non-linear optimizations required to reach a given accuracy of the fundamental matrix. Accuracy is measured using symmetric epipolar distance with respect to ground-truth points. The best hypothesis is selected every 1000 RANSAC iterations, and is further optimized using nonlinear (LM) method. In each dataset, the number of optimizations is averaged over all cameras pairs. Empty cells indicate that the required accuracy was not attained. <ref type="figure">Figure 8</ref>. The ratio between our method and Sinha <ref type="bibr" target="#b26">[27]</ref> of the number of non-linear optimization procedures required to reach a given fundamental matrix accuracy. The horizontal axis is the accuracy in terms of the desired symmetric epipolar distance of ground truth points.</p><p>is selected and optimized using the non-linear Levenberg-Marquardt (LM) optimization procedure as in <ref type="bibr" target="#b26">[27]</ref>. The efficiency is measured by the number of non-linear optimization procedures required to reach a given accuracy (error). The less non-linear optimization procedures the more efficient the method is. A detailed description is in Subsection 6.3. There is a difference in the error used during RANSAC and the error we use for final evaluation. During RANSAC, the quality of an hypothesis is evaluated based on inliers, as ground truth is unknown. This error is usually lower from the error of ground truth points. We used ground-truth points for a non biased evaluation.</p><p>Efficiency The expected number of non-linear LM opti-  mization procedures required to reach a fundamental matrix having a better accuracy than a predefined level is shown in <ref type="table">Table 2</ref>. For each pair of cameras, we executed 500K RANSAC iterations resulting in 500K hypotheses. Every 1000 RANSAC iterations the best hypothesis is selected and optimized non-linearly. The accuracy of the optimized fundamental matrix, in terms of the symmetric epipolar distance of ground truth points, is recorded. The accuracy values after all non-linear optimization procedures from all camera pairs in the dataset form our samples. For example, in the Kung-Fu dataset we executed 500K×300 RANSAC iterations, performed 500×300 LM optimizations, and collected 150,000 samples. We build the cumulative distribution function (cdf) of the error from all camera pairs. Given the cdf the expected number of samples is extracted. It can be seen that our method quickly converged to sub-pixel accuracy. <ref type="figure">Fig. 8</ref> shows the ratio between the required number of non-linear optimization procedures in our method and Sinha <ref type="bibr" target="#b26">[27]</ref>. The horizontal dashed lines are in ratios of 10, 30 and 100. For accuracy of 0.8 pixel, the median of the ratios between the required number of non-linear optimization procedures is 38, and for accuracy of 1.5 pixel the median of the ratios is 17. Accuracy We evaluated the best accuracy (minimal error) reached for a given number of RANSAC generated hypotheses. For each pair of cameras in the dataset, we generated 500K RANSAC hypotheses by each method. We subdivided the hypotheses into equal sized groups. From each group we selected the best hypothesis (lowest symmetric epipolar distance) with respect to the ground truth points. We then applied non-linear optimization and measured the accuracy of the resulting fundamental matrix. The accu- racy is the median over all optimized fundamental matrices. For example, in the Kung-Fu dataset we have 150,000 hypotheses. For evaluation of the highest accuracy reached by 5K hypotheses, we divided them into 30 equal size groups, optimized the best hypothesis from each group and evaluated the median over the symmetric epipolar distances. <ref type="table">Table 3</ref> shows the results. It can be seen that for the Kung-Fu dataset, our method requires approximately 2K RANSAC iterations followed with 1 non-linear optimization procedure to reach an accuracy of 0.85. Using our approach, in less than 5K RANSAC iterations all datasets reached subpixel accuracy. <ref type="table" target="#tab_2">Table 4</ref> shows the best median accuracy reached by each method. As expected, the synthetic dataset has best accuracy, 0.26, while the worst accuracy, 0.41, was in the Dancing Girl dataset which has many errors in the silhouettes.</p><formula xml:id="formula_5">(a) (b) (c) (d)</formula><p>We also evaluated the fraction of the number of camera pairs whose fundamental matrices reached a given accuracy using all the samples, after the non-linear phase. The results are shown in <ref type="figure" target="#fig_5">Fig. 9</ref>. For the Kung-Fu dataset, for 298 out of 300 camera pairs the accuracy reached 1.5, including pairs where the cameras are facing each other. This is discussed in the next subsection. On average, the number of camera pairs where a given accuracy was reached using our method is by a factor of 1.8 higher than the number of cameras with same accuracy using Sinha's method. The average is calculated over all camera pairs over all datasets. <ref type="figure">Figure 10</ref>. When the epipole is at the center of the image, e.g. when two cameras are facing one another, it may not be possible to find epipolar lines. In this case the epipole is often inside the convex hall. In this example the convex hull is marked in blue, and the yellow point is the epipole. The red line is a ground truth epipolar line. The green line is an hypothesized epipolar line.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Frames Lacking Frontier Points</head><p>Frames that lack frontier points are problematic for most tangent based methods. This happens when the epipoles are inside the convex hull of the dynamic objects, a common case when the two cameras face each other. An example is illustrated in <ref type="figure">Fig. 10</ref>. Using our method, even when the pairs of cameras are facing each other, the fundamental matrix can still be recovered. This is possible as the object is moving, and there are often a few frames where the epipole if outside the convex hall. These few frames are enough for the calibration, due to the accuracy of the selected candidates for epipolar lines. For example, it can be seen in <ref type="figure" target="#fig_5">Fig. 9</ref> that in the Kung-Fu dataset, for accuracy of 1.5, our method fails for only two camera pairs, whereas Sinha's method fails on 78 camera pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Ground-Truth Error vs. Inlier Error</head><p>The symmetric epipolar distance <ref type="bibr" target="#b16">[17]</ref> is a quality measure for fundamental matrices, and is defined over a set of pairs of corresponding points across two images.</p><p>In ordinary computations of the fundamental matrix, when no ground truth data is known, the symmetric epipolar distance is calculated based on hypothesized inlier points. Since some inliers are often wrong correspondences, there is a significant difference between the error computed on inlier points and the error computed on ground truth points (when available). As we have access to the ground truth in our datasets, we used the ground truth points to measure the symmetric epipolar distance and evaluate our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Implementation Details</head><p>Precomputation of Motion Barcodes. The motion barcodes were computed for points on the silhouette boundaries which are also on the convex hull boundary, called candidate points. 180 angles are sampled every 2 • , each angle defines a tangent line to the silhouette through one of the candidate points. This results in 180 candidate lines per frame, and a motion barcode is computed for all these lines. In a video having N frames, each motion barcode is a binary sequence of length N . A barcode matrix is defined for each frame having 180 rows and N columns. Each column represents a frame, and each row represents a tangent line. Each row is the motion barcode of the corresponding candidate line. Given corresponding frames of two cameras frames, the distance between all possible pairs of candidate lines is computed by multiplying their motion barcode matrices, resulting in an 180×180 affinity matrix of candidate lines.</p><p>Given the N pairs of frames of two cameras, we extract for each frame the single pair of candidate lines having the highest barcode correlation. This results in N pairs of possible matching epipolar lines, each having higher barcode correlation.</p><p>RANSAC Sampling. The efficiency of fundamental matrix computation can be broken into the initialization cost, the number of hypotheses needed to be generated, the cost of generating an hypothesis and the cost of hypothesis verification. In both methods the cost of the model verification phase is identical as it is indifferent to the model generation. The comparison is therefore the number of RANSAC hypotheses required by each method. In the following we provide a detailed description.</p><p>Generating the hypothesized model is as follows. In our method, three matching pairs having high barcode correlation were randomly selected from the pre-computed table of barcode correlations between all pairs of candidate lines. For the Sinha method, as described in <ref type="bibr" target="#b26">[27]</ref>, two matching hypothesized lines were extracted based on sampling the directions of the tangents in one frame. The third matching pair of lines was computed using the epipole generated by the first two lines, and a tangent to a silhouette in another frame. Given three proposals for corresponding epipolar lines, the fundamental matrix was computed using the method described in <ref type="bibr" target="#b26">[27]</ref>. The computation of the third matching pair of lines by the generated epipoles could be applied in our approach as well, requiring selection of only two matching lines instead of three. This could improve the accuracy of the method. On the other hand, it requires additional computations for finding the exact tangents in each RANSAC iteration. We empirically saw that sampling three lines is faster than sampling two lines together with the additional tangent computations.</p><p>The cost of each RANSAC iteration depends on (a) lines match generation and (b) the computation of the fundamental matrix from the epipolar line homography and the epipoles. For the motion barcode method the first part is instantaneous as it involves only index selection, since matching pairs of lines are computed beforehand. For the baseline method each iteration introduces the computation of six tangents, where the computation of the last pair of tangents involves finding the frontier points with respect to the hypothesized epipoles. The second part is the same for all the methods and introduces the major cost of each iteration. We assume that the first part is instantaneous also in the baseline method and consider the cost of each iteration as the cost of the second part.</p><p>Computing the motion barcode distance between all pairs of candidate lines adds computation efforts to our method. This computational cost was equivalent to 35 iterations of RANSAC, which we added to the cost of our method.</p><p>Ground Truth points. For accurate evaluation of the symmetric epipolar distance we extracted matching frontier points across different views, using the given ground-truth silhouettes and the given ground truth fundamental matrix. For each frame, points whose tangent line is within an angular deviation of 1 • of the true epipolar line were extracted. A pair of points was considered frontier if their epipolar distance using the known fundamental matrix is less than 0.01. This results in a cloud of points that might be spread out unevenly. From these points we sampled ground truth points that have a distance of at least 15 pixels from each other, resulting in several dozen point, well spread out, per view.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Concluding Remarks</head><p>Motion barcodes were introduced as efficient temporal signatures for lines, signatures which are viewpoint invariant for matching epipolar lines. The effectiveness of motion barcodes was demonstrated in camera calibration using candidate epipolar lines. In this case, computing candidate fundamental matrices only from candidate lines that have matching motion barcodes, reduced computational costs by about two orders of magnitude.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 .</head><label>3</label><figDesc>In dynamic scenes, the geometrical relation between pixels is characterized only up to corresponding epipolar lines. Each pixel in one video can correspond to different pixels at different times in the other video. For stationary cameras, the different pixels in the second view will always reside on the epipolar line.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>The motion barcodes of two corresponding epipolar lines, l and l ′ , in a video of a moving person at three time instances. If a point on an epipolar line is a projection of a foreground point at time t, then there exists a point on its corresponding epipolar line which is also a projection of a foreground point at the same time. In the figure, at time t = 2, 3 the two corresponding epipolar lines contain a point from a silhouette. This can be a different 3D point due to viewpoint differences, e.g. P1, P2. The motion barcode of both epipolar lines in this figure, b l and b l ′ , is [0,1,1].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 .</head><label>5</label><figDesc>Uniform sampling of lines from the tangent envelope. (a) Lines sampled every 1 • , (b) Lines sampled every 4 •</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 .</head><label>6</label><figDesc>Finding corresponding epipolar lines by their motion barcode. Every pair of frames contributes one possible match. The dashed lines are the true epipolar lines and the green lines are the candidate pairs having highest barcode correlation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 .</head><label>7</label><figDesc>The datasets used in the experiments. (a) The synthetic Kung-Fu girl dataset. (b) The Boxer dataset. (c) The Street Dancer dataset (d) The Dancing Girl dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 9 .</head><label>9</label><figDesc>The fraction of camera pairs whose fundamental matrices reached a given symmetric epipolar distance. The accuracy is evaluated over 500K RANSAC iterations. The x-axis is the given accuracy. The y-axis is the fraction of camera pairs that reached this accuracy. The blue bars are our method and the red bars are Sinha's method.(a) The Kung-Fu dataset. (b) Boxer dataset. (c) Street Dancer. (d) Dancing Girl.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 4 .</head><label>4</label><figDesc>The best accuracy reached by each method on all camera pairs in each dataset after 500K RANSAC hypotheses. The accuracy is the median over all camera pairs of the best symmetric epipolar distance reached after the non-linear optimization phase.</figDesc><table></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment.</head><p>This research was supported by Google, by Intel ICRI-CI, by DFG, and by the Israel Science Foundation.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<ptr target="http://4drepository.inrialpes.fr/public/viewgroup/1,2010.5" />
		<title level="m">Dancer dataset, inria, 4d-repository</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<ptr target="http://www.mpi-inf.mpg.de/depart-ments/irg3/kungfu/" />
		<title level="m">Data set, kung-fu girl</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Spatiotemporal shape from silhouette using four-dimensional delaunay meshing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Aganj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-P</forename><surname>Pons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ségonne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Keriven</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV&apos;07</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multimodal 3d shape recovery from texture, silhouette and shadow information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ballan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Cortelazzo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3D Data Processing, Visualization, and Transmission, Third International Symposium on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="924" to="930" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Event retrieval using motion barcodes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ben-Artzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Werman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Peleg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP&apos;15</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2621" to="2625" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">On using silhouettes for camera calibration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Boyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV&apos;06</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Shape-from-silhouette of articulated objects and its use for human body kinematics estimation and motion capture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR&apos;03</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">77</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Motion from the frontier of curved surfaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">E</forename><surname>Astrom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Giblin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV&apos;95</title>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="page" from="269" to="275" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Visual motion of curves and surfaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Giblin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Characterizing the uncertainty of the fundamental matrix</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Csurka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zeller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">D</forename><surname>Faugeras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVIU&apos;</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="18" to="36" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Cameraprojector matching using an unstructured video stream</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-A</forename><surname>Drouin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-M</forename><surname>Jodoin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Prémont</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR&apos;10 Workshop</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="33" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Activity based matching in distributed camera networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">B</forename><surname>Ermis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Clarot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-M</forename><surname>Jodoin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Saligrama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. IP</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2595" to="2613" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Fischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Bolles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="381" to="395" />
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Shape-fromsilhouette with two mirrors and an uncalibrated camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Forbes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Nicolls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">De</forename><surname>Jager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Voigt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV&apos;06</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="165" to="178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Exact polyhedral visual hulls</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-S</forename><surname>Franco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Boyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC&apos;03</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="329" to="338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Robust structure and motion from outlines of smooth curved surfaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Furukawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sethi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kriegman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. PAMI</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="302" to="315" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Multiple view geometry in computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Silhouette coherence for camera calibration under circular motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hernández</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schmitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. PAMI</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="343" to="349" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The information in temporal histograms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hoshen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Peleg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE WACV&apos;15</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="924" to="930" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The visual hull concept for silhouette-based image understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Laurentini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. PAMI</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="150" to="162" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">On computing exact visual hulls of solids bounded by smooth surfaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Boyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR&apos;01</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">156</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Epipolar geometry from profiles under circular motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">R S</forename><surname>Mendonca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-Y</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cippolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. PAMI</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="604" to="616" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Exact view-dependent visual-hulls</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hilton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR&apos;06</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="107" to="111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Curve matching and stereo calibration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Porrill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pollard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="45" to="50" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Silhouettebased multiple-view camera calibration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">G</forename><surname>Steinbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Girod</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VMV</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="3" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The geometry and matching of curves in multiple views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV&apos;98</title>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="394" to="409" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Camera network calibration and synchronization from silhouettes in archived video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">N</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Surface capture for performancebased animation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Starck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hilton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics and Applications</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="21" to="31" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A new evaluation criterion for point correspondences in stereo images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Stojanovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Unger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Analysis, Retrieval and Delivery of Multimedia Content</title>
		<editor>N. Adami, A. Cavallaro, R. Leonardi, and P. Migliorati</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="183" to="202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Structure and motion from silhouettes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-Y</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV&apos;01</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="217" to="222" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Epipolar geometry in stereo, motion and object recognition: a unified approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996" />
			<publisher>Springer Science &amp; Business Media</publisher>
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Multiple camera calibration with bundled optimization using silhouette geometry constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yamazoe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Utsumi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Abe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR&apos;06</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="960" to="963" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Self-calibration of turntable sequences from silhouettes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-Y</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. PAMI</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5" to="14" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
